{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporate RAG for Asset Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SetUp the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "import time\n",
    "from llmware.library import Library\n",
    "from llmware.retrieval import Query\n",
    "from llmware.prompts import Prompt\n",
    "from llmware.setup import Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test with an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test library and load it with llmware samples\n",
    "test_library = Library().create_new_library(\"Agreements\")\n",
    "samples_path = Setup().load_sample_files()\n",
    "test_library.add_files(os.path.join(samples_path,\"Agreements\"))\n",
    "\n",
    "# Create vector embeddings for the library and store them in Milvus\n",
    "test_library.install_new_embedding(embedding_model_name=\"industry-bert-contracts\", vector_db=\"milvus\")\n",
    "\n",
    "# Perform a semantic search in the test library\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # HuggingFace tokenizer warning to be avoided\n",
    "\n",
    "# Construct and perform a test query\n",
    "test_query = 'Resignation'\n",
    "query_res = Query(test_library).semantic_query(test_query, result_count=20)\n",
    "print(query_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a library and load it with llmware samples\n",
    "library = Library().create_new_library(\"Project_lib\")\n",
    "library.add_files('/Users/gerasimosplegas/Desktop/bling-rag-llm-project/docs')\n",
    "\n",
    "# Create vector embeddings for the library and store them in Milvus\n",
    "# Opt for the industry-bert-asset-management model which is trained for our domain\n",
    "library.install_new_embedding(embedding_model_name=\"industry-bert-asset-management\", vector_db=\"milvus\")\n",
    "\n",
    "# Construct the query\n",
    "query = 'What is defined as criticality?'\n",
    "query_res = Query(library).semantic_query(query, result_count=2)\n",
    "print(query_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text = ''\n",
    "for q in query_res:\n",
    "   embedded_text += '\\n'.join(q['text'].split(\"\\'\\'\"))\n",
    "\n",
    "\n",
    "# check all of the pertinent HuggingFace models for performance\n",
    "models = [\"llmware/bling-1b-0.1\",\n",
    "             \"llmware/bling-1.4b-0.1\",\n",
    "             \"llmware/bling-falcon-1b-0.1\",\n",
    "             \"llmware/bling-cerebras-1.3b-0.1\",\n",
    "             \"llmware/bling-sheared-llama-1.3b-0.1\",\n",
    "             \"llmware/bling-sheared-llama-2.7b-0.1\",\n",
    "             \"llmware/bling-red-pajamas-3b-0.1\",\n",
    "             ]\n",
    "\n",
    "\n",
    "model_list_small = [\n",
    "             \"llmware/bling-1b-0.1\",\n",
    "             ]\n",
    "\n",
    "\n",
    "# iterate through each model, prompt them and get the answer\n",
    "for model in models:\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n > Loading Model: {model}...\")\n",
    "    prompter = Prompt().load_model(model, from_hf=True, api_key=\"\")\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f\"\\n > Model {model} load time: {t1-t0} seconds\")\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    output = prompter.prompt_main(query, context=embedded_text\n",
    "                                 , prompt_name=\"default_with_context\",temperature=0.0)\n",
    "    \n",
    "    llm_response = output[\"llm_response\"].strip(\"\\n\")\n",
    "    print(f\"\\n > LLM Response: {llm_response}\")\n",
    "    print(f\"\\n > LLM Usage: {output['usage']}\")\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(f\"\\nTotal processing time: {t2-t1} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark against OpenAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API Key, either by setting the env var or editing it directly here:\n",
    "openai_api_key = ['YOUR_OPENAI_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new prompter using any desired model (GPT-3.5)and add the query_results\n",
    "prompt_text = \"Summarize the criticality provisions\"\n",
    "print (f\"\\n > Prompting LLM with '{prompt_text}'\")\n",
    "prompter = Prompt().load_model(\"gpt-3.5-turbo\", api_key=openai_api_key)\n",
    "sources = prompter.add_source_query_results(query_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the LLM with the sources and query string\n",
    "responses = prompter.prompt_with_source(prompt_text, prompt_name=\"summarize_with_bullets\")\n",
    "for response in responses:\n",
    "    print (\"\\n > LLM response\\n\" + response[\"llm_response\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
